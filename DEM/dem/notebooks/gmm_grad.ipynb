{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dem.energies.gmm_pseudoenergy import GMMPseudoEnergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "# Only initialize if not already initialized\n",
    "if not GlobalHydra().is_initialized():\n",
    "    # Initialize hydra with the same config path as train.py\n",
    "    hydra.initialize(config_path=\"../../configs\", version_base=\"1.3\")\n",
    "    # Load the experiment config for GMM with pseudo-energy\n",
    "    cfg = hydra.compose(config_name=\"train\", overrides=[\"experiment=gmm_idem_pseudo\"])\n",
    "\n",
    "# Instantiate the energy function using hydra, similar to train.py\n",
    "energy_function = hydra.utils.instantiate(cfg.energy)\n",
    "\n",
    "energy_function.gmm_potential.gmm.to(energy_function.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base = torch.tensor([0.1, 0.1], device=energy_function.gmm_potential.gmm.device)\n",
    "x_batch_base = torch.tensor([[0.0, 0.0], [1.0, 1.0]], device=energy_function.gmm_potential.gmm.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No derivatives in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential: tensor(-14.8496, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "potential_fn = lambda x: energy_function.gmm_potential(x)\n",
    "\n",
    "_x = x_base.clone()\n",
    "print(\"potential:\", potential_fn(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[-186.8610,  122.7425],\n",
      "        [-440.4272, -426.3380]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# vmapped grad (as used in estimating the score DEM/dem/models/components/score_estimator.py)\n",
    "grad_fxn = torch.func.grad(potential_fn, argnums=0)\n",
    "vmapped_fxn = torch.vmap(grad_fxn, in_dims=(0), randomness=\"different\")\n",
    "\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\", vmapped_fxn(_x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces (grad) in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential with grad: tensor(144.6745, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def potential_with_grad_fn(x): \n",
    "    energy = energy_function.gmm_potential(x)\n",
    "    forces = -torch.func.grad(energy_function.gmm_potential)(x)\n",
    "    force_magnitude = torch.norm(forces)\n",
    "    return energy + force_magnitude\n",
    "\n",
    "_x = x_base.clone()\n",
    "print(\"potential with grad:\", potential_with_grad_fn(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[1018.4152, -677.0424],\n",
      "        [ 601.0931,  581.8644]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# vmapped grad (as used in estimating the score DEM/dem/models/components/score_estimator.py)\n",
    "grad_fxn_grad = torch.func.grad(potential_with_grad_fn, argnums=0)\n",
    "vmapped_fxn_grad = torch.vmap(grad_fxn_grad, in_dims=(0), randomness=\"different\")\n",
    "\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\", vmapped_fxn_grad(_x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces (grad) and Hessian in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential with grad and hessian: tensor(2194.6680, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def potential_with_grad_and_hessian_fn(x):\n",
    "    energy = energy_function.gmm_potential(x)\n",
    "    grad = torch.func.grad(energy_function.gmm_potential)(x)\n",
    "    hessian = torch.func.hessian(energy_function.gmm_potential)(x)\n",
    "    return energy + torch.norm(grad) + torch.norm(hessian)\n",
    "\n",
    "_x = x_base.clone()\n",
    "print(\"potential with grad and hessian:\", potential_with_grad_and_hessian_fn(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[-1894.3495, -2516.1553],\n",
      "        [  601.0931,   581.8644]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# vmapped grad (as used in estimating the score DEM/dem/models/components/score_estimator.py)\n",
    "grad_fxn_hessian = torch.func.grad(potential_with_grad_and_hessian_fn, argnums=0)\n",
    "vmapped_fxn_hessian = torch.vmap(grad_fxn_hessian, in_dims=(0), randomness=\"different\")\n",
    "\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\", vmapped_fxn_hessian(_x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces (grad) and smallest Hessian eigenvalue in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential with grad and hessian eigenvalue: tensor(1594.2388, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def potential_with_grad_and_hessian_ev_fn(x):\n",
    "    energy = energy_function.gmm_potential(x)\n",
    "    grad = torch.func.grad(energy_function.gmm_potential)(x)\n",
    "    hessian = torch.func.hessian(energy_function.gmm_potential)(x)\n",
    "    \n",
    "    if hessian.shape[0] > 2:\n",
    "        # Get smallest 2 eigenvalues using LOBPCG\n",
    "        k = 2  # Number of eigenvalues to compute\n",
    "        init_X = torch.randn(hessian.shape[0], k, device=hessian.device)  # Initial guess\n",
    "        eigenvalues, _ = torch.lobpcg(hessian, k=k, largest=False, X=init_X)\n",
    "        smallest_eigenvalues = eigenvalues[:k]  # Get k smallest eigenvalues\n",
    "    \n",
    "    else:\n",
    "        # Get eigenvalues using torch.linalg.eigvals since Hessian is small\n",
    "        eigenvalues = torch.linalg.eigvals(hessian)\n",
    "        # Sort eigenvalues in ascending order\n",
    "        eigenvalues = torch.sort(eigenvalues.real)[0]  # Take real part and sort\n",
    "        smallest_eigenvalues = eigenvalues[:2]  # Get 2 smallest eigenvalues\n",
    "    \n",
    "    # Bias toward index-1 saddle points:\n",
    "    # - First eigenvalue should be negative (minimize positive values)\n",
    "    # - Second eigenvalue should be positive (minimize negative values) \n",
    "    # Using softplus which is differentiable everywhere but still creates one-sided penalties\n",
    "    ev1_bias = torch.nn.functional.softplus(smallest_eigenvalues[0])  # Penalize if first eigenvalue > 0\n",
    "    ev2_bias = torch.nn.functional.softplus(-smallest_eigenvalues[1])  # Penalize if second eigenvalue < 0\n",
    "    saddle_bias = ev1_bias + ev2_bias\n",
    "    \n",
    "    return energy + torch.norm(grad) + saddle_bias\n",
    "\n",
    "\n",
    "_x = x_base.clone()\n",
    "print(\"potential with grad and hessian eigenvalue:\", potential_with_grad_and_hessian_ev_fn(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[-1894.3495, -2516.1553],\n",
      "        [  601.0931,   581.8644]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# vmapped grad (as used in estimating the score DEM/dem/models/components/score_estimator.py)\n",
    "grad_fxn_hessian_ev = torch.func.grad(potential_with_grad_and_hessian_fn, argnums=0)\n",
    "vmapped_fxn_hessian_ev = torch.vmap(grad_fxn_hessian_ev, in_dims=(0), randomness=\"different\")\n",
    "\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\", vmapped_fxn_hessian_ev(_x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces (grad) and smallest Hessian eigenvalues (using Lanczos) in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lanczos loop includes a branch that depends on a tensor value (the check if b < 1e-8). When you use vmap, any dataâ€dependent control flow (i.e. branching on tensor values) is not supported. One common fix is to remove early termination based on the norm bb and instead always run a fixed number of iterations. \n",
    "\"\"\"\n",
    "\n",
    "def hessian_vector_product(grad, params, vec):\n",
    "    \"\"\"\n",
    "    Computes Hessian-vector product using autograd.\n",
    "    grad: gradient of the loss with respect to params.\n",
    "    params: list of parameters (or flattened tensor) on which the Hessian is defined.\n",
    "    vec: a flattened vector (same size as the concatenated params).\n",
    "    \"\"\"\n",
    "    # Compute the dot product between grad and vec\n",
    "    grad_dot = torch.dot(torch.cat([g.view(-1) for g in grad]), vec)\n",
    "    # Compute second derivative (Hessian-vector product)\n",
    "    hvp = torch.autograd.grad(grad_dot, params, retain_graph=True, create_graph=True)\n",
    "    return torch.cat([h.contiguous().view(-1) for h in hvp])\n",
    "\n",
    "def lanczos(Hv, v0, m, reg=1e-6):\n",
    "    \"\"\"\n",
    "    Lanczos algorithm to approximate a symmetric matrix's eigenvalues.\n",
    "    \n",
    "    Args:\n",
    "        Hv: a function that takes a vector and returns the Hessian-vector product.\n",
    "        v0: initial vector (1D tensor)\n",
    "        m: number of Lanczos iterations (fixed to ensure compatibility with vmap)\n",
    "        reg: regularization parameter for Hessian-vector product. Add small regularization to diagonal of Hessian (default=1e-6)\n",
    "        \n",
    "    Returns:\n",
    "        T: the tridiagonal matrix (m_actual x m_actual)\n",
    "        Q: orthonormal Lanczos basis (n x m_actual)\n",
    "    \"\"\"\n",
    "    Q = []\n",
    "    alpha = []\n",
    "    beta = []\n",
    "\n",
    "    # Normalize initial vector\n",
    "    q = v0 / v0.norm()\n",
    "    Q.append(q)\n",
    "\n",
    "    for j in range(m):\n",
    "        # Compute w = H*q_j - beta_{j-1} * q_{j-1} (skip the term for j == 0)\n",
    "        w = Hv(Q[j]) + reg * Q[j]  # Add regularization\n",
    "        if j > 0:\n",
    "            w = w - beta[j-1] * Q[j-1]\n",
    "        \n",
    "        # Compute alpha_j = q_j^T * w\n",
    "        a = torch.dot(Q[j], w)\n",
    "        alpha.append(a)\n",
    "        \n",
    "        # Orthogonalize w against q_j\n",
    "        w = w - a * Q[j]\n",
    "        \n",
    "        # Full reorthogonalization against all previous vectors\n",
    "        for k in range(j+1):\n",
    "            w = w - torch.dot(Q[k], w) * Q[k]\n",
    "            \n",
    "        # Compute beta_j = norm(w)\n",
    "        b = w.norm()\n",
    "        beta.append(b)\n",
    "        \n",
    "        if j == m - 1:\n",
    "            break\n",
    "            \n",
    "        # Safe division with larger epsilon\n",
    "        q_next = w / (b + 1e-6)\n",
    "        Q.append(q_next)\n",
    "        \n",
    "    m_actual = len(alpha)\n",
    "    # Build the tridiagonal matrix T from alpha and beta\n",
    "    T = torch.diag(torch.stack(alpha))\n",
    "    for i in range(m_actual - 1):\n",
    "        T[i, i+1] = beta[i+1]\n",
    "        T[i+1, i] = beta[i+1]\n",
    "    # Stack the basis vectors as columns in a matrix\n",
    "    Q_mat = torch.stack(Q, dim=1)\n",
    "    return T, Q_mat\n",
    "\n",
    "def compute_hessian_eigs(loss, params, lanczos_steps=100, reg=1e-6):  # Increased steps\n",
    "    # Compute first derivatives with create_graph for higher-order derivatives.\n",
    "    grad = torch.autograd.grad(loss, params, create_graph=True)\n",
    "    grad_flat = torch.cat([g.contiguous().view(-1) for g in grad])\n",
    "    \n",
    "    # Define Hessian-vector product function using the current grads and params.\n",
    "    def Hv(v):\n",
    "        return hessian_vector_product(grad, params, v)\n",
    "    \n",
    "    # Choose a random initial vector matching grad_flat's shape.\n",
    "    v0 = torch.randn_like(grad_flat)\n",
    "    \n",
    "    # Run the fixed-iteration Lanczos algorithm.\n",
    "    T, Q_mat = lanczos(Hv, v0, m=lanczos_steps, reg=reg)\n",
    "    \n",
    "    # Compute eigenvalues of the tridiagonal matrix T.\n",
    "    eigvals, _ = torch.linalg.eigh(T)\n",
    "    \n",
    "    # Return the smallest two eigenvalues.\n",
    "    smallest_two = eigvals[:2]\n",
    "    return smallest_two\n",
    "\n",
    "# Example potential function using functorch for Hessian-vector products:\n",
    "def potential_with_grad_and_hessian_lanczos_fn(x, lanczos_steps=100, reg=1e-6):\n",
    "    energy = energy_function.gmm_potential(x)\n",
    "    grad = torch.func.grad(energy_function.gmm_potential)(x)\n",
    "    \n",
    "    # Define loss function (here, simply the energy).\n",
    "    def loss_fn(x):\n",
    "        return energy_function.gmm_potential(x)\n",
    "    \n",
    "    # Get Hessian-vector product using functorch transforms.\n",
    "    grad_fn = torch.func.grad(loss_fn)\n",
    "    def hvp(v):\n",
    "        # Ensure v has the same shape as x.\n",
    "        v = v.reshape(x.shape)\n",
    "        return torch.func.jvp(grad_fn, (x,), (v,))[1]\n",
    "    \n",
    "    # Run Lanczos on the HVP function with increased iterations\n",
    "    v0 = torch.randn_like(x)\n",
    "    T, Q_mat = lanczos(hvp, v0, m=lanczos_steps)  # Increased from 40\n",
    "    \n",
    "    # Compute eigenvalues of the tridiagonal matrix.\n",
    "    smallest_eigenvalues, _ = torch.linalg.eigh(T)\n",
    "    smallest_eigenvalues = smallest_eigenvalues[:2]\n",
    "    \n",
    "    # Bias toward index-1 saddle points:\n",
    "    # - Penalize if the first eigenvalue is positive.\n",
    "    # - Penalize if the second eigenvalue is negative.\n",
    "    ev1_bias = torch.nn.functional.softplus(smallest_eigenvalues[0])\n",
    "    ev2_bias = torch.nn.functional.softplus(-smallest_eigenvalues[1])\n",
    "    saddle_bias = ev1_bias + ev2_bias\n",
    "    \n",
    "    return energy + torch.norm(grad) + saddle_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential with grad and hessian (Lanczos): tensor(1592.6531, device='cuda:0')\n",
      "grad:\n",
      " tensor([[-2240.0139, -2734.5093],\n",
      "        [  601.0931,   581.8644]], device='cuda:0')\n",
      "comparison grad:\n",
      " tensor([[-1894.3495, -2516.1553],\n",
      "        [  601.0931,   581.8644]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the function (ensure that energy_function and x_base, x_batch_base are defined in your context)\n",
    "_x = x_base.clone()\n",
    "print(\"potential with grad and hessian (Lanczos):\", potential_with_grad_and_hessian_lanczos_fn(_x))\n",
    "\n",
    "# vmapped grad:\n",
    "grad_fxn_hessian_lanczos = torch.func.grad(potential_with_grad_and_hessian_lanczos_fn, argnums=0)\n",
    "vmapped_fxn_hessian_lanczos = torch.vmap(grad_fxn_hessian_lanczos, in_dims=0, randomness=\"different\")\n",
    "\n",
    "\"\"\"\n",
    "The first row corresponds to x=[0,0] and the second to x=[1,1] in the batch. \n",
    "The second row is stable because at [1,1] the Hessian eigenvalues are well-conditioned, making the Lanczos approximation consistent. \n",
    "At [0,0], the system is near a critical point where eigenvalues are more sensitive to the random initialization.\n",
    "\"\"\"\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\\n\", vmapped_fxn_hessian_lanczos(_x_batch))\n",
    "\n",
    "# compare to potential_with_grad_and_hessian_ev_fn\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"comparison grad:\\n\", vmapped_fxn_hessian_ev(_x_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lanczos steps  | Max gradient difference\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd2181ad2cd4ed98a99b0359c102c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=       5 | diff=492.5\n",
      "steps=      10 | diff=631.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=      20 | diff=159.4\n",
      "steps=      50 | diff=59.6\n",
      "steps=      70 | diff=557.5\n",
      "steps=     100 | diff=329.0\n",
      "steps=     150 | diff=393.4\n",
      "No convergence achieved with tested step sizes (diff < 1.0)\n",
      "\n",
      "Lanczos steps  | Max gradient difference\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efecafa1976f41b2a8b66c302a1218b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=       5 | diff=719.9\n",
      "steps=      10 | diff=1218.7\n",
      "steps=      20 | diff=441.7\n",
      "steps=      50 | diff=1015.8\n",
      "steps=      70 | diff=1165.4\n",
      "steps=     100 | diff=685.5\n",
      "steps=     150 | diff=482.5\n",
      "No convergence achieved with tested step sizes (diff < 1.0)\n",
      "\n",
      "Lanczos steps  | Max gradient difference\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb32a022b78d4848bb1bc20defbc1124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=       5 | diff=721.8\n",
      "steps=      10 | diff=550.0\n",
      "steps=      20 | diff=14.7\n",
      "steps=      50 | diff=722.1\n",
      "steps=      70 | diff=465.3\n",
      "steps=     100 | diff=1116.2\n",
      "steps=     150 | diff=63.6\n",
      "No convergence achieved with tested step sizes (diff < 1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for minimum number of Lanczos steps needed for convergence\n",
    "\n",
    "# Compare to reference gradient\n",
    "comparison = vmapped_fxn_hessian_ev(x_batch_base.clone())\n",
    "\n",
    "def get_grad_diff(steps, reg):\n",
    "    # Create function with specific number of steps\n",
    "    def potential_fn(x):\n",
    "        return potential_with_grad_and_hessian_lanczos_fn(x, lanczos_steps=steps, reg=reg)\n",
    "    \n",
    "    # Get vmapped gradient\n",
    "    grad_fn = torch.func.grad(potential_fn, argnums=0)\n",
    "    vmapped_fn = torch.vmap(grad_fn, in_dims=0, randomness=\"different\")\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad = vmapped_fn(x_batch_base.clone())\n",
    "    \n",
    "    # Return max absolute difference\n",
    "    return torch.max(torch.abs(grad - comparison)).item()\n",
    "\n",
    "def do_step_search(reg=1e-6):\n",
    "    # Try increasing numbers of steps\n",
    "    step_sizes = [5, 10, 20, 50, 70, 100, 150]\n",
    "    diffs = []\n",
    "\n",
    "    print(\"Lanczos steps  | Max gradient difference\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Find minimum steps needed for reasonable convergence\n",
    "    threshold = 1.0  # Maximum acceptable difference\n",
    "    min_steps = None\n",
    "    for steps in tqdm(step_sizes):\n",
    "        diff = get_grad_diff(steps, reg)\n",
    "        diffs.append(diff)\n",
    "        if diff < threshold:\n",
    "            min_steps = steps\n",
    "            break\n",
    "        tqdm.write(f\"steps={steps:8d} | diff={diff:.1f}\")\n",
    "\n",
    "    if min_steps is not None:\n",
    "        print(f\"Minimum steps needed for diff < {threshold}: {min_steps}\")\n",
    "    else:\n",
    "        print(f\"No convergence achieved with tested step sizes (diff < {threshold})\\n\")\n",
    "\n",
    "do_step_search(reg=1e-6)\n",
    "do_step_search(reg=0.)\n",
    "do_step_search(reg=1e-12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
