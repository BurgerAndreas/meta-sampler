{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dem.energies.gmm_pseudoenergy import GMMPseudoEnergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "# Only initialize if not already initialized\n",
    "if not GlobalHydra().is_initialized():\n",
    "    # Initialize hydra with the same config path as train.py\n",
    "    hydra.initialize(config_path=\"../../configs\", version_base=\"1.3\")\n",
    "    # Load the experiment config for GMM with pseudo-energy\n",
    "    cfg = hydra.compose(config_name=\"train\", overrides=[\"experiment=gmm_idem_pseudo\"])\n",
    "\n",
    "# Instantiate the energy function using hydra, similar to train.py\n",
    "energy_function = hydra.utils.instantiate(cfg.energy)\n",
    "\n",
    "energy_function.gmm_potential.gmm.to(energy_function.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base = torch.tensor([0.1, 0.1], device=energy_function.gmm_potential.gmm.device)\n",
    "x_batch_base = torch.tensor([[0.0, 0.0], [1.0, 1.0]], device=energy_function.gmm_potential.gmm.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No derivatives in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential: tensor(-14.8496, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "potential_fn = lambda x: energy_function.gmm_potential(x)\n",
    "\n",
    "_x = x_base.clone()\n",
    "print(\"potential:\", potential_fn(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[-186.8610,  122.7425],\n",
      "        [-440.4272, -426.3380]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# vmapped grad (as used in estimating the score DEM/dem/models/components/score_estimator.py)\n",
    "grad_fxn = torch.func.grad(potential_fn, argnums=0)\n",
    "vmapped_fxn = torch.vmap(grad_fxn, in_dims=(0), randomness=\"different\")\n",
    "\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\", vmapped_fxn(_x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces (grad) in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential with grad: tensor(144.6745, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def potential_with_grad_fn(x): \n",
    "    energy = energy_function.gmm_potential(x)\n",
    "    forces = -torch.func.grad(energy_function.gmm_potential)(x)\n",
    "    force_magnitude = torch.norm(forces)\n",
    "    return energy + force_magnitude\n",
    "\n",
    "_x = x_base.clone()\n",
    "print(\"potential with grad:\", potential_with_grad_fn(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[1018.4152, -677.0424],\n",
      "        [ 601.0931,  581.8644]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# vmapped grad (as used in estimating the score DEM/dem/models/components/score_estimator.py)\n",
    "grad_fxn_grad = torch.func.grad(potential_with_grad_fn, argnums=0)\n",
    "vmapped_fxn_grad = torch.vmap(grad_fxn_grad, in_dims=(0), randomness=\"different\")\n",
    "\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\", vmapped_fxn_grad(_x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces (grad) and Hessian in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential with grad and hessian: tensor(2194.6680, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def potential_with_grad_and_hessian_fn(x):\n",
    "    energy = energy_function.gmm_potential(x)\n",
    "    grad = torch.func.grad(energy_function.gmm_potential)(x)\n",
    "    hessian = torch.func.hessian(energy_function.gmm_potential)(x)\n",
    "    return energy + torch.norm(grad) + torch.norm(hessian)\n",
    "\n",
    "_x = x_base.clone()\n",
    "print(\"potential with grad and hessian:\", potential_with_grad_and_hessian_fn(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[-1894.3495, -2516.1553],\n",
      "        [  601.0931,   581.8644]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# vmapped grad (as used in estimating the score DEM/dem/models/components/score_estimator.py)\n",
    "grad_fxn_hessian = torch.func.grad(potential_with_grad_and_hessian_fn, argnums=0)\n",
    "vmapped_fxn_hessian = torch.vmap(grad_fxn_hessian, in_dims=(0), randomness=\"different\")\n",
    "\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\", vmapped_fxn_hessian(_x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces (grad) and smallest Hessian eigenvalue in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential with grad and hessian eigenvalue: tensor(1594.2388, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def potential_with_grad_and_hessian_ev_fn(x):\n",
    "    energy = energy_function.gmm_potential(x)\n",
    "    grad = torch.func.grad(energy_function.gmm_potential)(x)\n",
    "    hessian = torch.func.hessian(energy_function.gmm_potential)(x)\n",
    "    \n",
    "    if hessian.shape[0] > 2:\n",
    "        # Get smallest 2 eigenvalues using LOBPCG\n",
    "        k = 2  # Number of eigenvalues to compute\n",
    "        init_X = torch.randn(hessian.shape[0], k, device=hessian.device)  # Initial guess\n",
    "        eigenvalues, _ = torch.lobpcg(hessian, k=k, largest=False, X=init_X)\n",
    "        smallest_eigenvalues = eigenvalues[:k]  # Get k smallest eigenvalues\n",
    "    \n",
    "    else:\n",
    "        # Get eigenvalues using torch.linalg.eigvals since Hessian is small\n",
    "        eigenvalues = torch.linalg.eigvals(hessian)\n",
    "        # Sort eigenvalues in ascending order\n",
    "        eigenvalues = torch.sort(eigenvalues.real)[0]  # Take real part and sort\n",
    "        smallest_eigenvalues = eigenvalues[:2]  # Get 2 smallest eigenvalues\n",
    "    \n",
    "    # Bias toward index-1 saddle points:\n",
    "    # - First eigenvalue should be negative (minimize positive values)\n",
    "    # - Second eigenvalue should be positive (minimize negative values) \n",
    "    # Using softplus which is differentiable everywhere but still creates one-sided penalties\n",
    "    ev1_bias = torch.nn.functional.softplus(smallest_eigenvalues[0])  # Penalize if first eigenvalue > 0\n",
    "    ev2_bias = torch.nn.functional.softplus(-smallest_eigenvalues[1])  # Penalize if second eigenvalue < 0\n",
    "    saddle_bias = ev1_bias + ev2_bias\n",
    "    \n",
    "    return energy + torch.norm(grad) + saddle_bias\n",
    "\n",
    "\n",
    "_x = x_base.clone()\n",
    "print(\"potential with grad and hessian eigenvalue:\", potential_with_grad_and_hessian_ev_fn(_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[-1894.3495, -2516.1553],\n",
      "        [  601.0931,   581.8644]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# vmapped grad (as used in estimating the score DEM/dem/models/components/score_estimator.py)\n",
    "grad_fxn_hessian_ev = torch.func.grad(potential_with_grad_and_hessian_fn, argnums=0)\n",
    "vmapped_fxn_hessian_ev = torch.vmap(grad_fxn_hessian_ev, in_dims=(0), randomness=\"different\")\n",
    "\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\", vmapped_fxn_hessian_ev(_x_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces (grad) and smallest Hessian eigenvalues (using Lanczos) in pseudo-potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lanczos loop includes a branch that depends on a tensor value (the check if b < 1e-8). When you use vmap, any data‐dependent control flow (i.e. branching on tensor values) is not supported. One common fix is to remove early termination based on the norm bb and instead always run a fixed number of iterations. \n",
    "\"\"\"\n",
    "\n",
    "def hessian_vector_product(grad, params, vec):\n",
    "    \"\"\"\n",
    "    Computes Hessian-vector product using autograd.\n",
    "    grad: gradient of the loss with respect to params.\n",
    "    params: list of parameters (or flattened tensor) on which the Hessian is defined.\n",
    "    vec: a flattened vector (same size as the concatenated params).\n",
    "    \"\"\"\n",
    "    # Compute the dot product between grad and vec\n",
    "    grad_dot = torch.dot(torch.cat([g.view(-1) for g in grad]), vec)\n",
    "    # Compute second derivative (Hessian-vector product)\n",
    "    hvp = torch.autograd.grad(grad_dot, params, retain_graph=True, create_graph=True)\n",
    "    return torch.cat([h.contiguous().view(-1) for h in hvp])\n",
    "\n",
    "def lanczos(Hv, v0, m, reg=1e-6):\n",
    "    \"\"\"\n",
    "    Lanczos algorithm to approximate a symmetric matrix's eigenvalues.\n",
    "    \n",
    "    Args:\n",
    "        Hv: a function that takes a vector and returns the Hessian-vector product.\n",
    "        v0: initial vector (1D tensor)\n",
    "        m: number of Lanczos iterations (fixed to ensure compatibility with vmap)\n",
    "        reg: regularization parameter for Hessian-vector product. Add small regularization to diagonal of Hessian (default=1e-6)\n",
    "        \n",
    "    Returns:\n",
    "        T: the tridiagonal matrix (m_actual x m_actual)\n",
    "        Q: orthonormal Lanczos basis (n x m_actual)\n",
    "    \"\"\"\n",
    "    Q = []\n",
    "    alpha = []\n",
    "    beta = []\n",
    "\n",
    "    # Normalize initial vector\n",
    "    q = v0 / v0.norm()\n",
    "    Q.append(q)\n",
    "\n",
    "    for j in range(m):\n",
    "        # Compute w = H*q_j - beta_{j-1} * q_{j-1} (skip the term for j == 0)\n",
    "        w = Hv(Q[j]) + reg * Q[j]  # Add regularization\n",
    "        if j > 0:\n",
    "            w = w - beta[j-1] * Q[j-1]\n",
    "        \n",
    "        # Compute alpha_j = q_j^T * w\n",
    "        a = torch.dot(Q[j], w)\n",
    "        alpha.append(a)\n",
    "        \n",
    "        # Orthogonalize w against q_j\n",
    "        w = w - a * Q[j]\n",
    "        \n",
    "        # Full reorthogonalization against all previous vectors\n",
    "        for k in range(j+1):\n",
    "            w = w - torch.dot(Q[k], w) * Q[k]\n",
    "            \n",
    "        # Compute beta_j = norm(w)\n",
    "        b = w.norm()\n",
    "        beta.append(b)\n",
    "        \n",
    "        if j == m - 1:\n",
    "            break\n",
    "            \n",
    "        # Safe division with larger epsilon\n",
    "        q_next = w / (b + 1e-6)\n",
    "        Q.append(q_next)\n",
    "        \n",
    "    m_actual = len(alpha)\n",
    "    # Build the tridiagonal matrix T from alpha and beta\n",
    "    T = torch.diag(torch.stack(alpha))\n",
    "    for i in range(m_actual - 1):\n",
    "        T[i, i+1] = beta[i+1]\n",
    "        T[i+1, i] = beta[i+1]\n",
    "    # Stack the basis vectors as columns in a matrix\n",
    "    Q_mat = torch.stack(Q, dim=1)\n",
    "    return T, Q_mat\n",
    "\n",
    "def compute_hessian_eigs(loss, params, lanczos_steps=100, reg=1e-6):  # Increased steps\n",
    "    # Compute first derivatives with create_graph for higher-order derivatives.\n",
    "    grad = torch.autograd.grad(loss, params, create_graph=True)\n",
    "    grad_flat = torch.cat([g.contiguous().view(-1) for g in grad])\n",
    "    \n",
    "    # Define Hessian-vector product function using the current grads and params.\n",
    "    def Hv(v):\n",
    "        return hessian_vector_product(grad, params, v)\n",
    "    \n",
    "    # Choose a random initial vector matching grad_flat's shape.\n",
    "    v0 = torch.randn_like(grad_flat)\n",
    "    \n",
    "    # Run the fixed-iteration Lanczos algorithm.\n",
    "    T, Q_mat = lanczos(Hv, v0, m=lanczos_steps, reg=reg)\n",
    "    \n",
    "    # Compute eigenvalues of the tridiagonal matrix T.\n",
    "    eigvals, _ = torch.linalg.eigh(T)\n",
    "    \n",
    "    # Return the smallest two eigenvalues.\n",
    "    smallest_two = eigvals[:2]\n",
    "    return smallest_two\n",
    "\n",
    "# Example potential function using functorch for Hessian-vector products:\n",
    "def potential_with_grad_and_hessian_lanczos_fn(x, lanczos_steps=100, reg=1e-6):\n",
    "    energy = energy_function.gmm_potential(x)\n",
    "    grad = torch.func.grad(energy_function.gmm_potential)(x)\n",
    "    \n",
    "    # Define loss function (here, simply the energy).\n",
    "    def loss_fn(x):\n",
    "        return energy_function.gmm_potential(x)\n",
    "    \n",
    "    # Get Hessian-vector product using functorch transforms.\n",
    "    grad_fn = torch.func.grad(loss_fn)\n",
    "    def hvp(v):\n",
    "        # Ensure v has the same shape as x.\n",
    "        v = v.reshape(x.shape)\n",
    "        return torch.func.jvp(grad_fn, (x,), (v,))[1]\n",
    "    \n",
    "    # Run Lanczos on the HVP function with increased iterations\n",
    "    v0 = torch.randn_like(x)\n",
    "    T, Q_mat = lanczos(hvp, v0, m=lanczos_steps)  # Increased from 40\n",
    "    \n",
    "    # Compute eigenvalues of the tridiagonal matrix.\n",
    "    smallest_eigenvalues, _ = torch.linalg.eigh(T)\n",
    "    smallest_eigenvalues = smallest_eigenvalues[:2]\n",
    "    \n",
    "    # Bias toward index-1 saddle points:\n",
    "    # - Penalize if the first eigenvalue is positive.\n",
    "    # - Penalize if the second eigenvalue is negative.\n",
    "    ev1_bias = torch.nn.functional.softplus(smallest_eigenvalues[0])\n",
    "    ev2_bias = torch.nn.functional.softplus(-smallest_eigenvalues[1])\n",
    "    saddle_bias = ev1_bias + ev2_bias\n",
    "    \n",
    "    return energy + torch.norm(grad) + saddle_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential with grad and hessian (Lanczos): tensor(1592.6531, device='cuda:0')\n",
      "grad:\n",
      " tensor([[-2240.0139, -2734.5093],\n",
      "        [  601.0931,   581.8644]], device='cuda:0')\n",
      "comparison grad:\n",
      " tensor([[-1894.3495, -2516.1553],\n",
      "        [  601.0931,   581.8644]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the function (ensure that energy_function and x_base, x_batch_base are defined in your context)\n",
    "_x = x_base.clone()\n",
    "print(\"potential with grad and hessian (Lanczos):\", potential_with_grad_and_hessian_lanczos_fn(_x))\n",
    "\n",
    "# vmapped grad:\n",
    "grad_fxn_hessian_lanczos = torch.func.grad(potential_with_grad_and_hessian_lanczos_fn, argnums=0)\n",
    "vmapped_fxn_hessian_lanczos = torch.vmap(grad_fxn_hessian_lanczos, in_dims=0, randomness=\"different\")\n",
    "\n",
    "\"\"\"\n",
    "The first row corresponds to x=[0,0] and the second to x=[1,1] in the batch. \n",
    "The second row is stable because at [1,1] the Hessian eigenvalues are well-conditioned, making the Lanczos approximation consistent. \n",
    "At [0,0], the system is near a critical point where eigenvalues are more sensitive to the random initialization.\n",
    "\"\"\"\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"grad:\\n\", vmapped_fxn_hessian_lanczos(_x_batch))\n",
    "\n",
    "# compare to potential_with_grad_and_hessian_ev_fn\n",
    "_x_batch = x_batch_base.clone()\n",
    "print(\"comparison grad:\\n\", vmapped_fxn_hessian_ev(_x_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lanczos steps  | Max gradient difference\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd2181ad2cd4ed98a99b0359c102c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=       5 | diff=492.5\n",
      "steps=      10 | diff=631.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=      20 | diff=159.4\n",
      "steps=      50 | diff=59.6\n",
      "steps=      70 | diff=557.5\n",
      "steps=     100 | diff=329.0\n",
      "steps=     150 | diff=393.4\n",
      "No convergence achieved with tested step sizes (diff < 1.0)\n",
      "\n",
      "Lanczos steps  | Max gradient difference\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efecafa1976f41b2a8b66c302a1218b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=       5 | diff=719.9\n",
      "steps=      10 | diff=1218.7\n",
      "steps=      20 | diff=441.7\n",
      "steps=      50 | diff=1015.8\n",
      "steps=      70 | diff=1165.4\n",
      "steps=     100 | diff=685.5\n",
      "steps=     150 | diff=482.5\n",
      "No convergence achieved with tested step sizes (diff < 1.0)\n",
      "\n",
      "Lanczos steps  | Max gradient difference\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb32a022b78d4848bb1bc20defbc1124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=       5 | diff=721.8\n",
      "steps=      10 | diff=550.0\n",
      "steps=      20 | diff=14.7\n",
      "steps=      50 | diff=722.1\n",
      "steps=      70 | diff=465.3\n",
      "steps=     100 | diff=1116.2\n",
      "steps=     150 | diff=63.6\n",
      "No convergence achieved with tested step sizes (diff < 1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for minimum number of Lanczos steps needed for convergence\n",
    "\n",
    "# Compare to reference gradient\n",
    "comparison = vmapped_fxn_hessian_ev(x_batch_base.clone())\n",
    "\n",
    "def get_grad_diff(steps, reg):\n",
    "    # Create function with specific number of steps\n",
    "    def potential_fn(x):\n",
    "        return potential_with_grad_and_hessian_lanczos_fn(x, lanczos_steps=steps, reg=reg)\n",
    "    \n",
    "    # Get vmapped gradient\n",
    "    grad_fn = torch.func.grad(potential_fn, argnums=0)\n",
    "    vmapped_fn = torch.vmap(grad_fn, in_dims=0, randomness=\"different\")\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad = vmapped_fn(x_batch_base.clone())\n",
    "    \n",
    "    # Return max absolute difference\n",
    "    return torch.max(torch.abs(grad - comparison)).item()\n",
    "\n",
    "def do_step_search(reg=1e-6):\n",
    "    # Try increasing numbers of steps\n",
    "    step_sizes = [5, 10, 20, 50, 70, 100, 150]\n",
    "    diffs = []\n",
    "\n",
    "    print(\"Lanczos steps  | Max gradient difference\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Find minimum steps needed for reasonable convergence\n",
    "    threshold = 1.0  # Maximum acceptable difference\n",
    "    min_steps = None\n",
    "    for steps in tqdm(step_sizes):\n",
    "        diff = get_grad_diff(steps, reg)\n",
    "        diffs.append(diff)\n",
    "        if diff < threshold:\n",
    "            min_steps = steps\n",
    "            break\n",
    "        tqdm.write(f\"steps={steps:8d} | diff={diff:.1f}\")\n",
    "\n",
    "    if min_steps is not None:\n",
    "        print(f\"Minimum steps needed for diff < {threshold}: {min_steps}\")\n",
    "    else:\n",
    "        print(f\"No convergence achieved with tested step sizes (diff < {threshold})\\n\")\n",
    "\n",
    "do_step_search(reg=1e-6)\n",
    "do_step_search(reg=0.)\n",
    "do_step_search(reg=1e-12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
